{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from collections import *\n",
        "\n",
        "\n",
        "class Node(object):\n",
        "  def __init__(self, depth, code, descr=None):\n",
        "    self.depth = depth\n",
        "    self.descr = descr or code\n",
        "    self.code = code\n",
        "    self.parent = None\n",
        "    self.children = []\n",
        "\n",
        "  def add_child(self, child):\n",
        "    if child not in self.children:\n",
        "      self.children.append(child)\n",
        "\n",
        "  def search(self, code):\n",
        "    if code == self.code: return [self]\n",
        "    ret = []\n",
        "    for child in self.children:\n",
        "      ret.extend(child.search(code))\n",
        "    return ret\n",
        "\n",
        "  def find(self, code):\n",
        "    nodes = self.search(code)\n",
        "    if nodes:\n",
        "      return nodes[0]\n",
        "    return None\n",
        "\n",
        "  @property\n",
        "  def root(self):\n",
        "    return self.parents[0]\n",
        "\n",
        "  @property\n",
        "  def description(self):\n",
        "    return self.descr\n",
        "\n",
        "  @property\n",
        "  def codes(self):\n",
        "    return map(lambda n: n.code, self.leaves)\n",
        "\n",
        "  @property\n",
        "  def parents(self):\n",
        "    n = self\n",
        "    ret = []\n",
        "    while n:\n",
        "      ret.append(n)\n",
        "      n = n.parent\n",
        "    ret.reverse()\n",
        "    return ret\n",
        "\n",
        "\n",
        "  @property\n",
        "  def leaves(self):\n",
        "    leaves = set()\n",
        "    if not self.children:\n",
        "      return [self]\n",
        "    for child in self.children:\n",
        "      leaves.update(child.leaves)\n",
        "    return list(leaves)\n",
        "\n",
        "  # return all leaf notes with a depth of @depth\n",
        "  def leaves_at_depth(self, depth):\n",
        "    return filter(lambda n: n.depth == depth, self.leaves)\n",
        "\n",
        "  @property\n",
        "  def siblings(self):\n",
        "    parent = self.parent\n",
        "    if not parent:\n",
        "      return []\n",
        "    return list(parent.children)\n",
        "\n",
        "  def __str__(self):\n",
        "    return '%s\\t%s' % (self.depth, self.code)\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash(str(self))\n",
        "\n",
        "\n",
        "class ICD9(Node):\n",
        "  def __init__(self, codesfname):\n",
        "    # dictionary of depth -> dictionary of code->node\n",
        "    self.depth2nodes = defaultdict(dict)\n",
        "    super(ICD9, self).__init__(-1, 'ROOT')\n",
        "\n",
        "    with open(\"/content/codes.json\", 'r') as f:\n",
        "      allcodes = json.loads(f.read())\n",
        "      self.process(allcodes)\n",
        "\n",
        "  def process(self, allcodes):\n",
        "    for hierarchy in allcodes:\n",
        "      self.add(hierarchy)\n",
        "\n",
        "  def get_node(self, depth, code, descr):\n",
        "    d = self.depth2nodes[depth]\n",
        "    if code not in d:\n",
        "      d[code] = Node(depth, code, descr)\n",
        "    return d[code]\n",
        "\n",
        "  def add(self, hierarchy):\n",
        "    prev_node = self\n",
        "    for depth, link in enumerate(hierarchy):\n",
        "      if not link['code']: continue\n",
        "\n",
        "      code = link['code']\n",
        "      descr = 'descr' in link and link['descr'] or code\n",
        "      node = self.get_node(depth, code, descr)\n",
        "      node.parent = prev_node\n",
        "      prev_node.add_child(node)\n",
        "      prev_node = node\n",
        "\n",
        "\n",
        "\n",
        "class SecondLevelCodes(object):\n",
        "    def __init__(self,icd_jon):\n",
        "        tree = ICD9(icd_jon)\n",
        "        # list of top level codes (e.g., '001-139', ...)\n",
        "        top_level_nodes = tree.children\n",
        "\n",
        "        # second level\n",
        "        self.second_level_codes = []\n",
        "        for node in top_level_nodes:\n",
        "            children = node.children\n",
        "            codes = [node.code for node in children]\n",
        "            self.second_level_codes.extend(codes)\n",
        "\n",
        "    def second_level_codes_icd9(self, dxStr):\n",
        "        code_3digit = convert_to_3digit_icd9(dxStr)\n",
        "        for code in self.second_level_codes:\n",
        "            if len(code) > 4:\n",
        "                codes = code.split('-')\n",
        "                if codes[0] <= code_3digit <= codes[1]:\n",
        "                    return code\n",
        "            elif code == code_3digit:\n",
        "                return code\n",
        "\n",
        "\n",
        "def convert_to_3digit_icd9(dxStr):\n",
        "    if dxStr.startswith('E'):\n",
        "        if len(dxStr) > 4:\n",
        "            return dxStr[:4]\n",
        "        else:\n",
        "            return dxStr\n",
        "    else:\n",
        "        if len(dxStr) > 3:\n",
        "            return dxStr[:3]\n",
        "        else:\n",
        "            return dxStr\n",
        "\n",
        "\n",
        "def convert_to_high_level_icd9(dxStr):\n",
        "\n",
        "    k = dxStr[:3]\n",
        "    if '001' <= k <= '139':\n",
        "        return 0\n",
        "    elif '140' <= k <= '239':\n",
        "        return 1\n",
        "    elif '240' <= k <= '279':\n",
        "        return 2\n",
        "    elif '280' <= k <= '289':\n",
        "        return 3\n",
        "    elif '290' <= k <= '319':\n",
        "        return 4\n",
        "    elif '320' <= k <= '389':\n",
        "        return 5\n",
        "    elif '390' <= k <= '459':\n",
        "        return 6\n",
        "    elif '460' <= k <= '519':\n",
        "        return 7\n",
        "    elif '520' <= k <= '579':\n",
        "        return 8\n",
        "    elif '580' <= k <= '629':\n",
        "        return 9\n",
        "    elif '630' <= k <= '679':\n",
        "        return 10\n",
        "    elif '680' <= k <= '709':\n",
        "        return 11\n",
        "    elif '710' <= k <= '739':\n",
        "        return 12\n",
        "    elif '740' <= k <= '759':\n",
        "        return 13\n",
        "    elif '760' <= k <= '779':\n",
        "        return 14\n",
        "    elif '780' <= k <= '799':\n",
        "        return 15\n",
        "    elif '800' <= k <= '999':\n",
        "        return 16\n",
        "    elif 'E00' <= k <= 'E99':\n",
        "        return 17\n",
        "    elif 'V01' <= k <= 'V90':\n",
        "        return 18\n",
        "\n",
        "\n",
        "class ICD_Ontology():\n",
        "    \n",
        "    def __init__(self,icd_file, dx_flag):\n",
        "        self.icd_file = icd_file\n",
        "        self.dx_flag = dx_flag\n",
        "        self.df = pd.read_csv(self.icd_file, index_col=0, dtype=object)\n",
        "        self.rootLevel()\n",
        "\n",
        "\n",
        "\n",
        "    def rootLevel(self):\n",
        "\n",
        "        dxs = self.df.ICD9_CODE.tolist()\n",
        "        dxMaps = dict()\n",
        "        \n",
        "        if self.dx_flag:\n",
        "            \n",
        "            for dx in dxs:\n",
        "                dxMaps.setdefault(dx[0:3], 0)\n",
        "\n",
        "            for k in dxMaps.keys():\n",
        "                if '001' <= k <= '139':\n",
        "                    dxMaps[k] = 1\n",
        "                if '140' <= k <= '239':\n",
        "                    dxMaps[k] = 2\n",
        "                if '240' <= k <= '279':\n",
        "                    dxMaps[k] = 3\n",
        "                if '280' <= k <= '289':\n",
        "                    dxMaps[k] = 4\n",
        "                if '290' <= k <= '319':\n",
        "                    dxMaps[k] = 5\n",
        "                if '320' <= k <= '389':\n",
        "                    dxMaps[k] = 6\n",
        "                if '390' <= k <= '459':\n",
        "                    dxMaps[k] = 7\n",
        "                if '460' <= k <= '519':\n",
        "                    dxMaps[k] = 8\n",
        "                if '520' <= k <= '579':\n",
        "                    dxMaps[k] = 9\n",
        "                if '580' <= k <= '629':\n",
        "                    dxMaps[k] = 10\n",
        "                if '630' <= k <= '679':\n",
        "                    dxMaps[k] = 11\n",
        "                if '680' <= k <= '709':\n",
        "                    dxMaps[k] = 12\n",
        "                if '710' <= k <= '739':\n",
        "                    dxMaps[k] = 13\n",
        "                if '740' <= k <= '759':\n",
        "                    dxMaps[k] = 14\n",
        "                if '760' <= k <= '779':\n",
        "                    dxMaps[k] = 15\n",
        "                if '780' <= k <= '799':\n",
        "                    dxMaps[k] = 16\n",
        "                if '800' <= k <= '999':\n",
        "                    dxMaps[k] = 17\n",
        "                if 'E00' <= k <= 'E99':\n",
        "                    dxMaps[k] = 18\n",
        "                if 'V01' <= k <= 'V90':\n",
        "                    dxMaps[k] = 19\n",
        "            self.rootMaps = dxMaps\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            for dx in dxs:\n",
        "                dxMaps.setdefault(dx[0:2], 0)\n",
        "\n",
        "            for k in dxMaps.keys():\n",
        "                if k == '00':\n",
        "                    dxMaps[k] = 1\n",
        "                if '01' <= k <= '05':\n",
        "                    dxMaps[k] = 2\n",
        "                if '06' <= k <= '07':\n",
        "                    dxMaps[k] = 3\n",
        "                if '08' <= k <= '16':\n",
        "                    dxMaps[k] = 4\n",
        "                if k == '17':\n",
        "                    dxMaps[k] = 5\n",
        "                if '18' <= k <= '20':\n",
        "                    dxMaps[k] = 6\n",
        "                if '21' <= k <= '29':\n",
        "                    dxMaps[k] = 7\n",
        "                if '30' <= k <= '34':\n",
        "                    dxMaps[k] = 8\n",
        "                if '35' <= k <= '39':\n",
        "                    dxMaps[k] = 9\n",
        "                if '40' <= k <= '41':\n",
        "                    dxMaps[k] = 10\n",
        "                if '42' <= k <= '54':\n",
        "                    dxMaps[k] = 11\n",
        "                if '55' <= k <= '59':\n",
        "                    dxMaps[k] = 12\n",
        "                if '60' <= k <= '64':\n",
        "                    dxMaps[k] = 13\n",
        "                if '65' <= k <= '71':\n",
        "                    dxMaps[k] = 14\n",
        "                if '72' <= k <= '75':\n",
        "                    dxMaps[k] = 15\n",
        "                if '76' <= k <= '84':\n",
        "                    dxMaps[k] = 16\n",
        "                if '85' <= k <= '86':\n",
        "                    dxMaps[k] = 17\n",
        "                if '87' <= k <= '99':\n",
        "                    dxMaps[k] = 18\n",
        "            dxMaps['E'] = 19\n",
        "            dxMaps['V'] = 20\n",
        "            self.rootMaps = dxMaps\n",
        "\n",
        "    def getRootLevel(self,code):\n",
        "        \n",
        "        if self.dx_flag:\n",
        "            root = code[0:3]\n",
        "        else:\n",
        "            if code.startswith('E'):\n",
        "                root = 'E'\n",
        "            elif code.startswith('V'):\n",
        "                root = 'V'\n",
        "            else:\n",
        "                root = code[0:2]\n",
        "        return self.rootMaps[root]\n",
        "\n",
        "\n",
        "class CCS_Ontology(object):\n",
        "    \n",
        "    def __init__(self, ccs_file):\n",
        "        self.ccs_file = ccs_file\n",
        "        self.rootLevel()\n",
        "        \n",
        "    def rootLevel(self):\n",
        "        # ccs_file = '../data/CCS/SingleDX-edit.txt'\n",
        "        with open(self.ccs_file) as f:\n",
        "            content = f.readlines()\n",
        "\n",
        "        pattern_code = '^\\w+'  # match code line in file\n",
        "        pattern_newline = '^\\n'  # match new line '\\n'\n",
        "\n",
        "        prog_code = re.compile(pattern_code)\n",
        "        prog_newline = re.compile(pattern_newline)\n",
        "\n",
        "        catIndex = 0\n",
        "        catMap = dict()  # store index:code list\n",
        "        codeList = list()\n",
        "        for line in content:\n",
        "            \n",
        "            # if the current line is code line, parse codes to a list and add to existing code list.\n",
        "            result_code = prog_code.match(line)\n",
        "            if result_code:\n",
        "                codes = line.split()\n",
        "                codeList.extend(codes)\n",
        "\n",
        "            # if current line is a new line, add new index and corresponding code list to the catMap dict.\n",
        "            result_newline = prog_newline.match(line)\n",
        "            if result_newline:\n",
        "                catMap[catIndex] = codeList\n",
        "                codeList = list()  # initualize the code list to empty\n",
        "                catIndex += 1  # next index\n",
        "                \n",
        "        code2CatMap = dict()\n",
        "        for key, value in catMap.items():\n",
        "            for code in value:\n",
        "                code2CatMap.setdefault(code, key)\n",
        "\n",
        "        self.rootMaps = code2CatMap\n",
        "    \n",
        "    def getRootLevel(self, code):\n",
        "        return self.rootMaps[code]\n"
      ],
      "metadata": {
        "id": "RMkNv5GMAYmN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "32e72084469253ba7b428e2d0bd46613",
          "grade": false,
          "grade_id": "cell-dcd6c662fba70926",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "fgXiZtFO270F"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:02.071632Z",
          "start_time": "2022-03-03T04:55:00.860211Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "478f107350e8fe54bd6fc439ffe45f4a",
          "grade": false,
          "grade_id": "cell-4fe346254a16fed8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "N6ZqDQ-4270G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from abc import ABCMeta\n",
        "import collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:02.083029Z",
          "start_time": "2022-03-03T04:55:02.076136Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5e279bf7debce23a92dd79a85312900e",
          "grade": false,
          "grade_id": "cell-6004290bcf81c83a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "aoErslm_270G"
      },
      "outputs": [],
      "source": [
        "# set seed\n",
        "seed = 24\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "DATA_PATH = \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "visit_threshold = 2\n",
        "\n",
        "with open(DATA_PATH+\"patients_mimic3_full.json\") as read_file:\n",
        "    patients = json.load(read_file)\n",
        "patients = [patient for patient in patients if len(patient['visits']) >= visit_threshold]"
      ],
      "metadata": {
        "id": "0VBb7q5442D3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "total_visits = 0\n",
        "\n",
        "dx_only = False\n",
        "min_freq = 5"
      ],
      "metadata": {
        "id": "wlzRqN055XtK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_codes = []  # store all diagnosis codes\n",
        "all_cpt_codes = []\n",
        "\n",
        "for patient in patients:\n",
        "    for visit in patient['visits']:\n",
        "        total_visits += 1\n",
        "        dxs = visit['DXs']\n",
        "        for dx in dxs:\n",
        "            all_codes.append('D_' + str(dx))\n",
        "        if not dx_only:\n",
        "            txs = visit['CPTs']\n",
        "            all_cpt_codes.extend(txs)\n",
        "            for tx in txs:\n",
        "                all_codes.append('T_' + str(tx))"
      ],
      "metadata": {
        "id": "P-dtIPI17Iq9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store all codes and corresponding counts\n",
        "count_org = []\n",
        "count_org.extend(collections.Counter(all_codes).most_common())"
      ],
      "metadata": {
        "id": "jV82kDlP91Lf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = []\n",
        "words_count = 0\n",
        "\n",
        "# store filtering codes and counts\n",
        "for word, c in count_org:\n",
        "    word_tuple = [word, c]\n",
        "    if c >= min_freq:\n",
        "        count.append(word_tuple)\n",
        "        words_count += c"
      ],
      "metadata": {
        "id": "uUdhcF439CkI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slc = SecondLevelCodes(\"/content/codes_pretty_printed.json\")\n",
        "second_level_codes = []\n",
        "# add padding\n",
        "dictionary = dict()\n",
        "code_to_second_level_code_dict=dict()\n",
        "dictionary_3digit = dict()\n",
        "dictionary['PAD'] = 0\n",
        "\n",
        "for word, cnt in count:\n",
        "    index = len(dictionary)\n",
        "    dictionary[word] = index\n",
        "    if word[:2] == 'D_':\n",
        "        digit = slc.second_level_codes_icd9(word[2:])\n",
        "        second_level_codes.append(digit)\n",
        "        code_to_second_level_code_dict[word] = digit\n",
        "\n",
        "count_second_level_codes = []\n",
        "count_second_level_codes.extend(collections.Counter(second_level_codes).most_common())\n",
        "for word, cnt in count_second_level_codes:\n",
        "    index = len(dictionary_3digit)\n",
        "    dictionary_3digit[word] = index"
      ],
      "metadata": {
        "id": "A6MhE26o-Q97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_len_visit = 0\n",
        "max_visits = 0\n",
        "\n",
        "\n",
        "for patient in patients:\n",
        "    visits = patient['visits']\n",
        "    len_visits = len(visits)\n",
        "    if len_visits > max_visits:\n",
        "        max_visits = len_visits\n",
        "    for visit in visits:\n",
        "        visit['Drugs'] = []\n",
        "        dxs = visit['DXs']\n",
        "        if len(dxs) == 0:\n",
        "            continue\n",
        "        else:\n",
        "            visit['DXs'] = [dictionary['D_' + str(dx)] for dx in dxs if 'D_' + str(dx) in dictionary]\n",
        "        len_current_visit = len(visit['DXs'])\n",
        "\n",
        "        if not dx_only:\n",
        "            txs = visit['CPTs']\n",
        "            visit['CPTs'] = [dictionary['T_' + str(tx)] for tx in txs if 'T_' + str(tx) in dictionary]\n",
        "            # len_current_visit = len(visit['DXs']+visit['CPTs'])\n",
        "        if len_current_visit > max_len_visit:\n",
        "            max_len_visit = len_current_visit\n",
        "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "\n",
        "with open(\"/content/mimic_dx.json\", 'w') as fp:\n",
        "    json.dump(reverse_dictionary, fp)\n",
        "\n",
        "# with open(self.patients_codes_file + '.json', 'w') as fp:\n",
        "#     json.dump(self.patients, fp)\n"
      ],
      "metadata": {
        "id": "BpHSG-jbCRWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_context_codes = None\n",
        "train_labels_1 = None\n",
        "train_labels_2 = None\n",
        "train_context_onehots = None\n",
        "dev_context_codes = None\n",
        "dev_labels_1 = None\n",
        "dev_labels_2 = None\n",
        "dev_context_onehots = None\n",
        "test_context_codes = None\n",
        "test_labels_1 = None\n",
        "test_labels_2 = None\n",
        "test_context_onehots = None\n",
        "train_size = 0\n",
        "train_pids = None\n",
        "dev_pids = None\n",
        "test_pids = None\n",
        "train_intervals = None\n",
        "dev_intervals = None\n",
        "test_intervals = None"
      ],
      "metadata": {
        "id": "dXfddK9YQMUJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_valid_visits = 10\n",
        "\n",
        "if not dx_only:\n",
        "    max_len_visit += 10\n",
        "    print(f\"max_len_visit:{max_len_visit}\")\n",
        "batches = []\n",
        "n_zeros = 0\n",
        "\n",
        "for patient in patients:\n",
        "    pid = patient['pid']\n",
        "    # get patient's visits\n",
        "    visits = patient['visits']\n",
        "    # sorting visits by admission date\n",
        "    sorted_visits = sorted(visits, key=lambda visit: visit['admsn_dt'])\n",
        "    valid_visits = []\n",
        "    for v in sorted_visits:\n",
        "        if len(v['DXs']) > 0 and sum(v['DXs']) > 0:\n",
        "            valid_visits.append(v)\n",
        "\n",
        "    if (len(valid_visits)) < 2:\n",
        "        continue\n",
        "\n",
        "    # number of visits and only use 10 visits to predict last one if number of visits is larger than 11\n",
        "    no_visits = len(valid_visits)\n",
        "    last_visit = valid_visits[no_visits - 1]\n",
        "    second_last_visit = valid_visits[no_visits - 2]\n",
        "\n",
        "    ls_codes = []\n",
        "    ls_intervals = []\n",
        "    # only use 10 visits to predict last one if number of visits is larger than 11\n",
        "    if no_visits > cfg_valid_visits+1:\n",
        "        feature_visits = valid_visits[no_visits-(cfg_valid_visits+1):no_visits-1]\n",
        "    else:\n",
        "        feature_visits = valid_visits[0:no_visits - 1]\n",
        "\n",
        "    n_visits = len(feature_visits)\n",
        "    # if n_visits == 0:\n",
        "    #     n_zeros += 1\n",
        "    first_valid_visit_dt = datetime.datetime.strptime(feature_visits[0]['admsn_dt'], \"%Y%m%d\")\n",
        "    for i in range(n_visits):\n",
        "        visit = feature_visits[i]\n",
        "        codes = visit['DXs']\n",
        "        if not dx_only:\n",
        "            length = len(visit['CPTs'])\n",
        "            if length < 11:\n",
        "                codes.extend(visit['CPTs'])\n",
        "            else:\n",
        "                codes.extend(visit['CPTs'][:10])\n",
        "\n",
        "        if sum(codes) == 0:\n",
        "            n_zeros += 1\n",
        "\n",
        "        current_dt = datetime.datetime.strptime(visit['admsn_dt'], \"%Y%m%d\")\n",
        "        interval = (current_dt - first_valid_visit_dt).days + 1\n",
        "        ls_intervals.append(interval)\n",
        "        code_size = len(codes)\n",
        "        # code padding\n",
        "        if code_size < max_len_visit:\n",
        "            list_zeros = [0] * (max_len_visit - code_size)\n",
        "            codes.extend(list_zeros)\n",
        "        ls_codes.append(codes)\n",
        "\n",
        "    # visit padding\n",
        "    if n_visits < cfg_valid_visits:\n",
        "        for i in range(cfg_valid_visits - n_visits):\n",
        "            list_zeros = [0] * max_len_visit\n",
        "            ls_codes.append(list_zeros)\n",
        "            ls_intervals.append(0)\n",
        "\n",
        "\n",
        "    last_dt = datetime.datetime.strptime(last_visit['admsn_dt'], \"%Y%m%d\")\n",
        "    second_last_dt = datetime.datetime.strptime(second_last_visit['admsn_dt'], \"%Y%m%d\")\n",
        "    days = (last_dt - second_last_dt).days\n",
        "    if days <= 30:\n",
        "        adm_label = 1\n",
        "    else:\n",
        "        adm_label = 0\n",
        "    # --------- end readmission label --------------------\n",
        "\n",
        "    # --------- second level category --------------------\n",
        "    one_hot_labels = np.zeros(len(dictionary_3digit)).astype(int)\n",
        "    last_codes = last_visit['DXs']\n",
        "    for code in last_codes:\n",
        "        code_str = reverse_dictionary[code]\n",
        "        cat_code = code_to_second_level_code_dict[code_str]\n",
        "        index = dictionary_3digit[cat_code]\n",
        "        one_hot_labels[index] = 1\n",
        "    # --------- end diagnosis label --------------------\n",
        "\n",
        "    # --------- high level icd9 diagnosis label --------------------\n",
        "    # one_hot_labels = np.zeros(19).astype(int)\n",
        "    # last_codes = last_visit['DXs']\n",
        "    # for code in last_codes:\n",
        "    #     code_str = self.reverse_dictionary[code]\n",
        "    #     index = convert_to_high_level_icd9(code_str[2:])\n",
        "    #     one_hot_labels[index] = 1\n",
        "    # --------- end diagnosis label --------------------\n",
        "    batches.append(\n",
        "        [np.array(ls_codes, dtype=np.int32), one_hot_labels, np.array([adm_label], dtype=np.int32), pid,\n",
        "            np.array(ls_intervals, dtype=np.int32)])\n",
        "\n",
        "print('number of non-context ', n_zeros)\n",
        "codes = []\n",
        "dx_labels = []\n",
        "re_labels = []\n",
        "pids = []\n",
        "intervals = []\n",
        "for batch in batches:\n",
        "    codes.append(batch[0])\n",
        "    dx_labels.append(batch[1])\n",
        "    re_labels.append(batch[2])\n",
        "    pids.append(batch[3])\n",
        "    intervals.append(batch[4])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w2QN42P2P8n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "212e434fd38be5ca223e82a1e1fddf5b",
          "grade": false,
          "grade_id": "cell-71f2f1fcbf0214c3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1uJXCxOr270G"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:02.129270Z",
          "start_time": "2022-03-03T04:55:02.123149Z"
        },
        "deletable": false,
        "id": "8r7shPPT270I"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, seqs, hfs):\n",
        "        self.x = seqs\n",
        "        self.y = hfs\n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        \"\"\"\n",
        "        TODO: Return the number of samples (i.e. patients).\n",
        "        \"\"\"\n",
        "        \n",
        "        # your code here\n",
        "        return len(self.x)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        \"\"\"\n",
        "        TODO: Generates one sample of data.\n",
        "        \n",
        "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
        "        \"\"\"\n",
        "        \n",
        "        # your code here\n",
        "        return self.x[index], self.y[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:02.143419Z",
          "start_time": "2022-03-03T04:55:02.132257Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9698147ad965e9a4336317e632e30259",
          "grade": true,
          "grade_id": "cell-cc0baa6c9dadef8c",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "0rkycMzg270I"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = CustomDataset(codes, np.array(re_labels).reshape(-1,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:02.157114Z",
          "start_time": "2022-03-03T04:55:02.146695Z"
        },
        "deletable": false,
        "id": "h_508XaW270I"
      },
      "outputs": [],
      "source": [
        "def collate_fn(data):\n",
        "    \"\"\"\n",
        "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
        "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
        "        is stored in `mask`.\n",
        "    \n",
        "    Arguments:\n",
        "        data: a list of samples fetched from `CustomDataset`\n",
        "        \n",
        "    Outputs:\n",
        "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
        "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
        "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
        "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
        "        y: a tensor of shape (# patiens) of type torch.float\n",
        "        \n",
        "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
        "        using: `sequences, labels = zip(*data)`\n",
        "    \"\"\"\n",
        "\n",
        "    sequences, labels = zip(*data)\n",
        "    sequences = list(sequences)\n",
        "    y = torch.tensor(labels, dtype=torch.float)\n",
        "    num_patients = len(y)\n",
        "    num_visits = [len(patient) for patient in sequences]\n",
        "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
        "    max_num_visits = max(num_visits)\n",
        "    max_num_codes = max(num_codes)\n",
        "    for i in range(len(sequences)):\n",
        "        sequences[i] = sequences[i][~np.all(sequences[i] == 0, axis=1)]\n",
        "    seqs = []\n",
        "    for i in sequences:\n",
        "        seq = []\n",
        "        for j in i:\n",
        "            seq.append([mm for mm in j if mm != 0])\n",
        "        seqs.append(seq)\n",
        "    sequences = seqs\n",
        "\n",
        "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
        "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
        "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
        "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
        "\n",
        "    for i_patient, patient in enumerate(sequences):\n",
        "            n_visit = len(patient)\n",
        "            for j_visit, visit in enumerate(patient):\n",
        "                pad_visit = F.pad(torch.tensor(visit), pad=(0, max_num_codes-len(visit)), mode='constant', value=0)\n",
        "                pad_mask = F.pad(torch.full((len(visit),), 1), pad=(0, max_num_codes-len(visit)), mode='constant', value=0)\n",
        "                x[i_patient, j_visit, : ] = pad_visit\n",
        "                rev_x[i_patient, n_visit-1-j_visit, : ] = pad_visit\n",
        "                masks[i_patient, j_visit, : ] = pad_mask\n",
        "                rev_masks[i_patient, n_visit-1-j_visit, : ] = pad_mask\n",
        "    return x, masks, rev_x, rev_masks, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:02.192838Z",
          "start_time": "2022-03-03T04:55:02.187126Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cc4a76fda00ecaef5a2e8857a49fb5f2",
          "grade": false,
          "grade_id": "cell-7f2e734b97c94232",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTnhJ0O2270J",
        "outputId": "0a9578ec-c361-45db-aaa8-090a3fbf0493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train dataset: 5992\n",
            "Length of val dataset: 1498\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "split = int(len(dataset)*0.8)\n",
        "\n",
        "lengths = [split, len(dataset) - split]\n",
        "train_dataset, val_dataset = random_split(dataset, lengths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:02.199440Z",
          "start_time": "2022-03-03T04:55:02.194423Z"
        },
        "deletable": false,
        "id": "04nNWqqH270J"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def load_data(train_dataset, val_dataset, collate_fn):\n",
        "    \n",
        "    \n",
        "    batch_size = 32\n",
        "    # your code here\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "    \n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:02.211023Z",
          "start_time": "2022-03-03T04:55:02.207187Z"
        },
        "deletable": false,
        "id": "CuYtUZzu270K"
      },
      "outputs": [],
      "source": [
        "class AlphaAttention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        \n",
        "        self.a_att = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, g):\n",
        "        \n",
        "        # your code here\n",
        "        alpha = self.a_att(g)\n",
        "        alpha = F.softmax(alpha, dim=1)\n",
        "        return alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:02.228622Z",
          "start_time": "2022-03-03T04:55:02.225083Z"
        },
        "deletable": false,
        "id": "WNIBHhHI270K"
      },
      "outputs": [],
      "source": [
        "class BetaAttention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.b_att = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "\n",
        "    def forward(self, h):\n",
        "        # your code here\n",
        "        beta = self.b_att(h)\n",
        "        beta = F.tanh(beta)\n",
        "        return beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:02.244533Z",
          "start_time": "2022-03-03T04:55:02.240331Z"
        },
        "deletable": false,
        "id": "yOqvnaKc270K"
      },
      "outputs": [],
      "source": [
        "def attention_sum(alpha, beta, rev_v, rev_masks):\n",
        "    # your code here\n",
        "    mask = torch.any(rev_masks, dim=-1).unsqueeze(-1)\n",
        "    y = torch.sum(alpha*beta*rev_v*mask, dim=1)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PnSuyrRtmdso"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:02.260356Z",
          "start_time": "2022-03-03T04:55:02.257005Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "657bb0ca386783a293d79ed9ef2b2c2c",
          "grade": false,
          "grade_id": "cell-b6589b3ccf6f9a92",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "lvAhJEuu270K"
      },
      "outputs": [],
      "source": [
        "def sum_embeddings_with_mask(x, masks):\n",
        "    \n",
        "    x = x * masks.unsqueeze(-1)\n",
        "    x = torch.sum(x, dim = -2)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:02.279333Z",
          "start_time": "2022-03-03T04:55:02.262313Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c702d8aa22420dc7dbe58264b6268b9c",
          "grade": false,
          "grade_id": "cell-02ad1b5432de58bd",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTkN1JAu270K",
        "outputId": "121933b1-6623-4263-f2d3-532020e5b358"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RETAIN(\n",
              "  (embedding): Embedding(3184, 150)\n",
              "  (rnn_a): GRU(150, 150, batch_first=True)\n",
              "  (rnn_b): GRU(150, 150, batch_first=True)\n",
              "  (att_a): AlphaAttention(\n",
              "    (a_att): Linear(in_features=150, out_features=1, bias=True)\n",
              "  )\n",
              "  (att_b): BetaAttention(\n",
              "    (b_att): Linear(in_features=150, out_features=150, bias=True)\n",
              "  )\n",
              "  (fc): Linear(in_features=150, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "class RETAIN(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_codes, embedding_dim=150):\n",
        "        super().__init__()\n",
        "        # Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
        "        self.embedding = nn.Embedding(num_codes, embedding_dim)\n",
        "        # Define the RNN-alpha using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
        "        self.rnn_a = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
        "        # Define the RNN-beta using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
        "        self.rnn_b = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
        "        # Define the alpha-attention using `AlphaAttention()`;\n",
        "        self.att_a = AlphaAttention(embedding_dim)\n",
        "        # Define the beta-attention using `BetaAttention()`;\n",
        "        self.att_b = BetaAttention(embedding_dim)\n",
        "        # Define the linear layers using `nn.Linear()`;\n",
        "        self.fc = nn.Linear(embedding_dim, 1)\n",
        "        # Define the final activation layer using `nn.Sigmoid().\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x, masks, rev_x, rev_masks):\n",
        "        # 1. Pass the reversed sequence through the embedding layer;\n",
        "        rev_x = self.embedding(rev_x)\n",
        "        # 2. Sum the reversed embeddings for each diagnosis code up for a visit of a patient.\n",
        "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
        "        # 3. Pass the reversed embegginds through the RNN-alpha and RNN-beta layer separately;\n",
        "        g, _ = self.rnn_a(rev_x)\n",
        "        h, _ = self.rnn_b(rev_x)\n",
        "        # 4. Obtain the alpha and beta attentions using `AlphaAttention()` and `BetaAttention()`;\n",
        "        alpha = self.att_a(g)\n",
        "        beta = self.att_b(h)\n",
        "        # 5. Sum the attention up using `attention_sum()`;\n",
        "        c = attention_sum(alpha, beta, rev_x, rev_masks)\n",
        "        # 6. Pass the context vector through the linear and activation layers.\n",
        "        logits = self.fc(c)\n",
        "        probs = self.sigmoid(logits)\n",
        "        return probs.squeeze()\n",
        "    \n",
        "\n",
        "retain = RETAIN(num_codes = np.array(codes).max())\n",
        "retain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8ed184b69aed0279933617da64ae7e19",
          "grade": false,
          "grade_id": "cell-873df7380d762445",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "nPPjI8xi270L"
      },
      "source": [
        "## 3 Training and Inferencing [10 points]\n",
        "\n",
        "Then, let us implement the `eval()` function first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:02.977330Z",
          "start_time": "2022-03-03T04:55:02.286119Z"
        },
        "deletable": false,
        "id": "9SEk86kz270L"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
        "\n",
        "\n",
        "def eval(model, val_loader):\n",
        "    \n",
        "    model.eval()\n",
        "    y_pred = torch.LongTensor()\n",
        "    y_score = torch.Tensor()\n",
        "    y_true = torch.LongTensor()\n",
        "    model.eval()\n",
        "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
        "        y_logit = model(x, masks, rev_x, rev_masks)\n",
        "        y_hat = y_logit > 0.5\n",
        "        y_score = torch.cat((y_score,  y_logit.detach().to('cpu')), dim=0)\n",
        "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
        "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
        "    \n",
        "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "    pr_auc = average_precision_score(y_true, y_score)\n",
        "    return p, r, f, pr_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2a52b20a062ffbd7f40dd59951eeb4ff",
          "grade": false,
          "grade_id": "cell-9b3672b70944a8d9",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "px7mxfv6270L"
      },
      "source": [
        "Now let us implement the `train()` function. Note that `train()` should call `eval()` at the end of each training epoch to see the results on the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:02.985668Z",
          "start_time": "2022-03-03T04:55:02.979447Z"
        },
        "deletable": false,
        "id": "VgNcOOQh270L"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, n_epochs):\n",
        "    \"\"\"\n",
        "    Train the model.\n",
        "    \n",
        "    Arguments:\n",
        "        model: the RNN model\n",
        "        train_loader: training dataloder\n",
        "        val_loader: validation dataloader\n",
        "        n_epochs: total number of epochs\n",
        "    \"\"\"\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(x, masks, rev_x, rev_masks)\n",
        "            \"\"\" \n",
        "            TODO: calculate the loss using `criterion`, save the output to loss.\n",
        "            \"\"\"\n",
        "            \n",
        "            # your code here\n",
        "            loss = criterion(y_hat,y)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "        p, r, f, roc_auc = eval(model, val_loader)\n",
        "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, pr_auc: {:.2f}'.format(epoch+1, p, r, f, roc_auc))\n",
        "    return round(roc_auc, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:05.442581Z",
          "start_time": "2022-03-03T04:55:02.987452Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0641300bf647c8eecbb44e65fa8ef306",
          "grade": false,
          "grade_id": "cell-2f0785a5f7faf51f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "-ds8BejU270L"
      },
      "outputs": [],
      "source": [
        "# load the model\n",
        "retain = RETAIN(num_codes = np.array(codes).max()+1)\n",
        "\n",
        "# load the loss function\n",
        "criterion = nn.BCELoss()\n",
        "# load the optimizer\n",
        "# optimizer = torch.optim.Adam(retain.parameters(), lr=1e-4)\n",
        "optimizer = torch.optim.RMSprop(retain.parameters(), lr=1e-4)\n",
        "\n",
        "n_epochs = 10\n",
        "train(retain, train_loader, val_loader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for name, module in retain.named_modules():\n",
        "#      print(name, sum(param.numel() for param in module.parameters()))"
      ],
      "metadata": {
        "id": "ATJtoOXbBO9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-03T04:55:13.323581Z",
          "start_time": "2022-03-03T04:55:05.503893Z"
        },
        "deletable": false,
        "id": "zKyfpIWP270L"
      },
      "outputs": [],
      "source": [
        "lr_hyperparameter = [1e-1, 1e-3]\n",
        "embedding_dim_hyperparameter = [8, 128]\n",
        "n_epochs = 5\n",
        "results = {}\n",
        "\n",
        "for lr in lr_hyperparameter:\n",
        "    for embedding_dim in embedding_dim_hyperparameter:\n",
        "        print ('='*50)\n",
        "        print ({'learning rate': lr, \"embedding_dim\": embedding_dim})\n",
        "        print ('-'*50)\n",
        "\n",
        "        # your code here\n",
        "        retain = RETAIN(num_codes = len(types), embedding_dim=embedding_dim)\n",
        "        criterion = nn.BCELoss()\n",
        "        # load the optimizer\n",
        "        optimizer = torch.optim.Adam(retain.parameters(), lr=lr)\n",
        "        \n",
        "        \n",
        "        \n",
        "        pr_auc = train(retain, train_loader, val_loader, n_epochs)\n",
        "        results['lr:{},emb:{}'.format(str(lr), str(embedding_dim))] =  pr_auc"
      ]
    }
  ],
  "metadata": {
    "illinois_payload": {
      "b64z": "",
      "nb_path": "release/HW4_RETAIN/HW4_RETAIN.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3 (Threads: 2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "852px",
        "left": "204px",
        "top": "110px",
        "width": "317.390625px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "RETAIN_Readm_Diagn.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}